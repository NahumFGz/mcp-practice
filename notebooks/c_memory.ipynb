{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Memoria con PostgreSQL\n",
    "https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "from langgraph.prebuilt import create_react_agent  # üëà construye agente ReAct listo\n",
    "from psycopg_pool import AsyncConnectionPool\n",
    "\n",
    "DB_HOST = config(\"DB_HOST\")\n",
    "DB_PORT = config(\"DB_PORT\")\n",
    "DB_NAME = config(\"DB_NAME\")\n",
    "DB_USER = config(\"DB_USER\")\n",
    "DB_PASSWORD = config(\"DB_PASSWORD\")\n",
    "\n",
    "DB_URI = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\" \"?sslmode=disable\"\n",
    "\n",
    "pool = AsyncConnectionPool(\n",
    "    conninfo=DB_URI,\n",
    "    max_size=10,\n",
    "    kwargs={\n",
    "        \"autocommit\": True,\n",
    "        \"prepare_threshold\": 0,  # evita errores con statements cacheados\n",
    "    },\n",
    ")\n",
    "checkpointer = AsyncPostgresSaver(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.checkpoint.postgres.aio.AsyncPostgresSaver at 0x1140d70a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Caching\n",
    "https://python.langchain.com/docs/integrations/caches/redis_llm_caching/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Redis at: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "REDIS_URL = config(\"REDIS_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.schema import Generation\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_redis import RedisCache, RedisSemanticCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:44:01 httpx INFO   HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "First call (not cached):\n",
      "Result: \n",
      "\n",
      "1. Definir el objetivo del proyecto: El primer paso en un proyecto de IA es definir claramente el objetivo que se quiere lograr. Esto ayudar√° a enfocar los esfuerzos y recursos en la direcci√≥n correcta.\n",
      "\n",
      "2. Recopilar y limpiar datos: La IA se basa en datos, por lo que es importante recopilar y limpiar datos relevantes y de alta calidad para el proyecto. Esto puede incluir datos estructurados (como bases de datos) y no estructurados (como im√°genes o texto).\n",
      "\n",
      "3. Seleccionar algoritmos y modelos: Una vez que se tienen los datos, se deben seleccionar los algoritmos y modelos adecuados para el proyecto. Esto depender√° del tipo de problema que se est√© abordando y de los datos disponibles.\n",
      "\n",
      "4. Entrenar y validar el modelo: El siguiente paso es entrenar el modelo seleccionado con los datos recopilados. Esto implica ajustar los par√°metros del modelo para que pueda realizar predicciones precisas. Luego, se debe validar el modelo para asegurarse de que est√© funcionando correctamente.\n",
      "\n",
      "5. Implementar el modelo: Una vez que el modelo\n",
      "Time: 4.74 seconds\n",
      "\n",
      "Second call (cached):\n",
      "Result: \n",
      "\n",
      "1. Definir el objetivo del proyecto: El primer paso en un proyecto de IA es definir claramente el objetivo que se quiere lograr. Esto ayudar√° a enfocar los esfuerzos y recursos en la direcci√≥n correcta.\n",
      "\n",
      "2. Recopilar y limpiar datos: La IA se basa en datos, por lo que es importante recopilar y limpiar datos relevantes y de alta calidad para el proyecto. Esto puede incluir datos estructurados (como bases de datos) y no estructurados (como im√°genes o texto).\n",
      "\n",
      "3. Seleccionar algoritmos y modelos: Una vez que se tienen los datos, se deben seleccionar los algoritmos y modelos adecuados para el proyecto. Esto depender√° del tipo de problema que se est√© abordando y de los datos disponibles.\n",
      "\n",
      "4. Entrenar y validar el modelo: El siguiente paso es entrenar el modelo seleccionado con los datos recopilados. Esto implica ajustar los par√°metros del modelo para que pueda realizar predicciones precisas. Luego, se debe validar el modelo para asegurarse de que est√© funcionando correctamente.\n",
      "\n",
      "5. Implementar el modelo: Una vez que el modelo\n",
      "Time: 0.00 seconds\n",
      "\n",
      "Speed improvement: 3178.69x faster\n",
      "Cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisCache\n",
    "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(redis_cache)\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "# Function to measure execution time\n",
    "def timed_completion(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# First call (not cached)\n",
    "prompt = \"Que acciones se pueden realizar en un proyecto de IA?\"\n",
    "result1, time1 = timed_completion(prompt)\n",
    "print(f\"First call (not cached):\\nResult: {result1}\\nTime: {time1:.2f} seconds\\n\")\n",
    "\n",
    "# Second call (should be cached)\n",
    "result2, time2 = timed_completion(prompt)\n",
    "print(f\"Second call (cached):\\nResult: {result2}\\nTime: {time2:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the cache\n",
    "redis_cache.clear()\n",
    "print(\"Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:47:09 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "03:47:09 redisvl.index.index INFO   Index already exists, not overwriting.\n",
      "03:47:10 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "03:47:11 httpx INFO   HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "03:47:12 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Original query:\n",
      "Prompt: Cual es la capital del Peru?\n",
      "Result: \n",
      "\n",
      "La capital del Per√∫ es Lima.\n",
      "Time: 3.21 seconds\n",
      "\n",
      "03:47:13 httpx INFO   HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Similar query:\n",
      "Prompt: Puedes decirme una ciudad del Peru?\n",
      "Result: \n",
      "\n",
      "La capital del Per√∫ es Lima.\n",
      "Time: 0.64 seconds\n",
      "\n",
      "Speed improvement: 5.00x faster\n",
      "Semantic cache cleared\n"
     ]
    }
   ],
   "source": [
    "# Initialize RedisSemanticCache\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_cache = RedisSemanticCache(\n",
    "    redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2\n",
    ")\n",
    "\n",
    "# Set the cache for LangChain to use\n",
    "set_llm_cache(semantic_cache)\n",
    "\n",
    "\n",
    "# Function to test semantic cache\n",
    "def test_semantic_cache(prompt):\n",
    "    start_time = time.time()\n",
    "    result = llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "\n",
    "# Original query\n",
    "original_prompt = \"Cual es la capital del Peru?\"\n",
    "result1, time1 = test_semantic_cache(original_prompt)\n",
    "print(\n",
    "    f\"Original query:\\nPrompt: {original_prompt}\\nResult: {result1}\\nTime: {time1:.2f} seconds\\n\"\n",
    ")\n",
    "\n",
    "# Semantically similar query\n",
    "similar_prompt = \"Puedes decirme una ciudad del Peru?\"\n",
    "result2, time2 = test_semantic_cache(similar_prompt)\n",
    "print(\n",
    "    f\"Similar query:\\nPrompt: {similar_prompt}\\nResult: {result2}\\nTime: {time2:.2f} seconds\\n\"\n",
    ")\n",
    "\n",
    "print(f\"Speed improvement: {time1 / time2:.2f}x faster\")\n",
    "\n",
    "# Clear the semantic cache\n",
    "semantic_cache.clear()\n",
    "print(\"Semantic cache cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mcptesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
